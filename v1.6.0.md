# Harvester v1.6.0 Release Notes

This release introduces several features, enhancements, and bug fixes that improve system quality and the overall user experience. The documentation is available at https://docs.harvesterhci.io/v1.6.

The Harvester team appreciates your contributions and looks forward to receiving feedback regarding this release.

## Downloads

### AMD64

#### Full ISO


#### Net Install ISO


### ARM64


## Installation

Harvester can be installed using the [ISO image](https://docs.harvesterhci.io/v1.6/install/index), a [bootable USB drive](https://docs.harvesterhci.io/v1.6/install/usb-install), and [PXE boot](https://docs.harvesterhci.io/v1.6/install/pxe-boot-install/). A [net install ISO image](https://docs.harvesterhci.io/v1.6/install/net-install), which contains only the core OS components, is also now available. For more information, see the [Installation](https://docs.harvesterhci.io/v1.6/install/requirements) section of the documentation.

> [!IMPORTANT]
> The Harvester v1.6.0 installer checks if the hardware meets the [minimum requirements](https://docs.harvesterhci.io/v1.6/install/requirements/#hardware-requirements) for production use. If any of the checks fail, installation is stopped and warnings are printed to the system console.
> 
> You can disable this behavior during iPXE installation (for testing purposes) by adding the kernel parameter `harvester.install.skipchecks=true` when you boot the system. For more information, see [Useful Kernel Parameters](https://docs.harvesterhci.io/v1.6/install/pxe-boot-install#harvesterinstallskipcheckstrue).

## Upgrades

Harvester only allows upgrades from supported versions. For more information about upgrade paths and procedures, see [Upgrading Harvester](https://docs.harvesterhci.io/v1.6/upgrade/index).

## Highlights

### Experimental Features

> [!IMPORTANT]
> All new features that use Kube-OVN are considered experimental. The Kube-OVN implementation in Harvester v1.6.0 does not support upgrades and virtual machine live migration.

#### Kube-OVN Operator

The `kubeovn-operator` add-on is a tool for managing Kube-OVN as a secondary container network interface (CNI) on Harvester clusters. Kube-OVN provides advanced software-defined networking (SDN) capabilities such as virtual private cloud (VPC) and subnets for virtual machine workloads. When you enable the add-on, it deploys Kube-OVN to your Harvester cluster and creates the default `Configuration` CRD, which defines the desired state of the Kube-OVN installation.

[Documentation](https://docs.harvesterhci.io/v1.6/advanced/addons/kubeovn-operator) | [GitHub issue](https://github.com/harvester/harvester/issues/7413)

#### Overlay Network

The Harvester network-controller leverages Kube-OVN to create an OVN-based virtualized network that supports advanced SDN capabilities such as VPCs and subnets for virtual machine workloads. An overlay network represents a virtual layer 2 switch that encapsulates and forwards traffic between virtual machines.

#### Virtual Private Cloud (VPC)



#### Kube-OVN Virtual Machine Isolation

Isolation between virtual machines is typically achieved using either VLANs (in traditional networks) or virtual switches (in Kube-OVN). If you want to isolate virtual machines within the same virtual switch network, you can use subnet access control lists (ACLs) and Kubernetes network policies to achieve the required micro-segmentation.

[Documentation](https://docs.harvesterhci.io/v1.6/networking/kubeovn-vm-isolation) | [GitHub issue](https://github.com/harvester/harvester/issues/7381)

### Fully Supported Features

#### CPU and Memory Hotplug

Starting with Harvester v1.6.0, you can increase a virtual machine's CPU and memory resources while it is running. To use this feature, you must enable CPU and memory hotplug when you create the virtual machine. Once the virtual machine is created, you can add more CPU and memory resources at any time. Harvester automatically migrates the virtual machine to a node that has the necessary resources.

[Demo](https://youtu.be/QujSXPH-SfI) | [Documentation](https://docs.harvesterhci.io/v1.6/vm/cpu-memory-hotplug) | [GitHub issue](https://github.com/harvester/harvester/issues/5835)

#### Online Volume Expansion

Harvester v1.6.0 allows you to expand volumes that are in use, as long as the underlying storage provider supports the feature. This works for volumes that are attached to a running virtual machine, or that have persistent volume claims (PVCs) connected to a running pod in a guest cluster.

The Longhorn V1 Data Engine fully supports online volume expansion, but the V2 Data Engine currently does not support it. For third-party storage, Harvester blocks online volume expansion requests by default. To allow such requests, you must use the `csi-online-expand-validation` setting to validate the storage provider.

[Demo](https://youtu.be/e273Ve_INjA) | [Documentation](https://docs.harvesterhci.io/v1.6/volume/edit-volume#online-volume-expansion) | GitHub issues: [2811](https://github.com/harvester/harvester/issues/2811) and [7358](https://github.com/harvester/harvester/issues/7358)

#### Third-Party Storage for Guest Clusters and Workloads

In Harvester v1.5.0, support for third-party storage is limited to provisioning of root and data volumes for virtual machines. Harvester v1.6.0 improves on this by also allowing you to provision guest clusters and their corresponding workloads with third-party storage solutions.

In your guest cluster, you must first create a new StorageClass that references the StorageClass you created in Harvester for the third-party storage solution. Then, you must create a PersistentVolumeClaim (PVC) using this new StorageClass and mount it to your workload.

[Demo](https://youtu.be/e273Ve_INjA) | GitHub issues: [8075](https://github.com/harvester/harvester/issues/8075) and [8076](https://github.com/harvester/harvester/issues/8076)

#### vm-import-controller Support for Third-Party CSIs

When importing virtual machine images using the vm-import-controller add-on, you can now specify a third-party storage solution instead of using Longhorn by default. To use this functionality, simply add the `storageClass` field to the `VirtualMachineImport` spec and specify the StorageClass for the third-party storage solution. Harvester handles the import and storage placement without requiring you to perform additional manual steps.

[Demo](https://youtu.be/nlyrs3jhl2g) | [Documentation](https://docs.harvesterhci.io/v1.6/advanced/addons/vmimport/) | [GitHub Issue](https://github.com/harvester/harvester/issues/8074)

#### VM Migration Network

By default, Harvester migrates virtual machines over the built-in cluster network `mgmt`, which is limited to one interface and shared with cluster-wide workloads. If network segregation is required, you can configure a VM migration network to isolate migration traffic and improve bandwidth utilization.

Configuring a VM migration network involves enabling the `vm-migration-network` setting and constructing a Multus `NetworkAttachmentDefinition` CRD. Once the setting is applied, all `virt-handler` pods are restarted to apply the new network configuration.

[Documentation](https://docs.harvesterhci.io/v1.6/advanced/vm-migration-network) | [GitHub issue](https://github.com/harvester/harvester/issues/5848)

## Features


## Enhancements


## Bug Fixes


## Others


## Known Issues


## Deprecations and Removals 

### Rancher Kubernetes Engine (RKE)

RKE reached its end-of-life on July 31, 2025. Because of this, support for RKE has been removed from Harvester v1.6.0. You will no longer be able to run Rancher-provisioned RKE clusters on this version or any future versions.

For better security and efficiency, you may want to replatform to RKE2. This process involves [creating new RKE2 clusters](https://docs.harvesterhci.io/v1.6/rancher/node/rke2-cluster) and migrating existing workloads from the old RKE clusters. In-place migration from RKE to RKE2 is not an option because of possible data loss. For more information, see [RKE End of Life](https://www.suse.com/support/kb/doc/?id=000021513).

### `install.wipedisks` setting

This setting was removed from Harvester v1.5. If you want to clear disk partitions with the `COS_OEM` label, use [`install.wipe_all_disks`](https://docs.harvesterhci.io/v1.6/install/harvester-configuration#installwipe_all_disks) and [`install.wipe_disks_list`](https://docs.harvesterhci.io/v1.6/install/harvester-configuration#installwipe_disks_list) instead.

## Components

| Component | Version |
| --- | --- |
| Longhorn | ? |
| KubeVirt | ? |
| Embedded Rancher | ? |
| RKE2 | ? |
| SLE Micro for Rancher | ? |

# Contributors

Thank you to all the contributors who made this release possible.

